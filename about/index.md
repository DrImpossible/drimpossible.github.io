---
title: ''
author: Ameya Prabhu
layout: page
permalink: "/about"
dsq_thread_id:
---
Table of Contents     [About Me](#biography)     [Highlights](#highlights)     [Research](#research)    
 [Projects](#projects)     [Publications](#publications)

---

## <a name="biography" id="biography"></a>About Me

Hi! I’m Ameya, and since you're reading this: a-may-yeah (as in __a__ book, __may__ I?, __yeah__ sure!). "Ameya" is the sanskrit word "अमेय" approximately translating to "Impossible". Just too full of energy as a baby, right? My parents went from "You're impossible" to "You're Ameya". This github username was created when in high school, as a battle against the impossible dream of being "Dr. Ameya" (pun intended) and motivation for it.

I like to think about **the future** in general, currently intrigued by intelligent assistance (IA), computational economics, automated theorem provers and constitutional law. I want to dedicate my career towards designing principled algorithms to acquire more general notions of "learnability" and help better automating tasks (like proving tedious theorems) and also assist humans for other stuff. Serious researchers think it's cute when I talk about most of the above: including price theory, mechanism design, etc. I'm nevertheless excited by these fields and their insights.

Speaking of homegrounds, I aspire to an the ideal of "theoretically grounded approaches should go hand-in-hand with challenging practical problems": Principled approaches can accelerate research compared to extensive hit-and-try approaches popular today. Conversely, it's important to make real-world assumptions to model computationally hard problems and side-step worst-case complexity. As [Marx](https://en.wikipedia.org/wiki/Groucho_Marx) once famously said "Those are my principles, and if you don't like them.. well, I have others".

On the fun side, the [jungian personality type](https://slatestarcodex.com/2014/05/27/on-types-of-typologies/) of the academic me has consistently been INTP, dominated by Ti and Ne; or my [MTG Color Wheel](https://medium.com/s/story/the-mtg-color-wheel-c9700a7cf36d) results are dominated by Blue and Red with a White tinge. Also, [a blog I liked](https://www.lesswrong.com/posts/wDP4ZWYLNj7MGXWiW/in-praise-of-fake-frameworks) for meta-discussion about fake frameworks. If this last paragraph interests you more than anything above it, here is the [non-academic me](https://bayesianconspirator.github.io/about/) where you might find more of the same. Welcome to my personal page folks!

## <a name="highlights" id="highlights"></a>Highlights

<table style="border-collapse: collapse; border: none; margin: 0px auto;" width="100%" align='left'> <tr style="border: none;"> <td style="border: none; "><h3>Education</h3></td><td style="border: none; "><h3>Experience</h3></td><td style="border: none; "><h3>Research</h3></td> <td style="border: none; "><h3>Code</h3></td> </tr> <tr> <td style="border: none; "> <img src="https://raw.githubusercontent.com/drimpossible/drimpossible.github.io/master/images/Oxford.png" height="60px" width="60px" /> <img src="https://raw.githubusercontent.com/drimpossible/drimpossible.github.io/master/images/IIITH.png" height="60px" width="60px" /> </td> <td style="border: none; "> <img src="https://raw.githubusercontent.com/drimpossible/drimpossible.github.io/master/images/IBM.png" height="60px" width="60px" /> <img src="https://raw.githubusercontent.com/drimpossible/drimpossible.github.io/master/images/Verisk.jpg" height="60px" width="60px" /></td> <td style="vertical-align: top;"><strong> <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper.pdf"> ECCV</a><br/> <a href="https://arxiv.org/abs/1804.03867">WACV</a><br/> <a href="https://aclanthology.info/papers/C16-1234/c16-1234">COLING</a><br/><br/> </strong></td> <td style="vertical-align: top;"><strong>
<a href="https://github.com/drimpossible/Deep-Expander-Networks"><img src="https://raw.githubusercontent.com/drimpossible/drimpossible.github.io/master/images/github.png" height="15px" width="15px">Deep Expander Networks</a><br/>
<a href="https://github.com/erilyth/HybridBinaryNetworks-WACV18"><img src="https://raw.githubusercontent.com/drimpossible/drimpossible.github.io/master/images/github.png" height="15px" width="15px">Hybrid Binary Networks</a><br/>
<a href="https://github.com/drimpossible/Sub-word-LSTM"><img src="https://raw.githubusercontent.com/drimpossible/drimpossible.github.io/master/images/github.png" height="15px" width="15px">Subword-LSTM</a><br/>
</strong></td>
</tr>
</table>

## <a name="achievements" id="achievements"></a>Achievements

### 2020

* Got oustanding reviewer award for ECCV 2020!
* My research on issues with continual learning [**Greedy Sampler and Dumb Learner: A Surprisingly Effective Approach for Continual Learning**]() got accepted at ECCV 2020 (Oral) (Acceptance rate: 2\%).
* Got outstanding reviewer award for CVPR 2020 (Acceptance rate: 4% :P)!

### 2019

* My research on active learning on text classification [**Sampling Bias in Deep Active Classification: An Empirical Study**](https://github.com/drimpossible/drimpossible.github.io/blob/master/documents/Active-Sampling-Bias.pdf) got accepted at EMNLP 2019.
* Completed my bachelors and masters in computer science at [IIIT-Hyderabad](http://www.iiit.ac.in). [**My thesis**](https://github.com/drimpossible/drimpossible.github.io/blob/master/documents/Thesis.pdf) focused on developing principled approaches for sparse connectivity (pruning) and methods to exploit fast-binary convolutions for building efficient but accurate deep networks.
* Got outstanding reviewer award for CVPR 2019! Here is the [reviewer guideline](/blog/life/reviewing_for_dummies/) I developed for reference.
* I'm pursuing a PhD in Machine Learning at University of Oxford, one small step towards achieving my big goal.

### 2018

* Got a yearlong residency at **Verisk HQ**, to work for the ML group of a NASDAQ 100 financial risk assessment firm.
* I attended [**ECCV 2018**](https://eccv2018.org/). Thank you [Google](https://ai.google/research/) for providing full travel-grant!
* My research on Pruning [**Deep Expander Networks: Efficient Networks from Graph Theory**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper.pdf) got accepted at ECCV 2018 (Oral).
* I completed a research internship at IBM Research to experience industrial research. Got Best Intern Award, Summer '18 for outstanding research at IBM.
* I attended [**WACV 2018**](http://wacv18.wacv.net/). Thank you [CVIT](http://cvit.iiit.ac.in/) for providing full-funding!
* My research on Hybrid Binary Networks [**Hybrid Binary Networks: Optimizing for Accuracy, Efficiency and Memory**](https://arxiv.org/abs/1804.03867) got accepted at WACV 2018 (Oral). 
* My research on Distribution-aware Binary Networks [**Distribution-Aware Binarization of Neural Networks for Sketch Recognition**](https://arxiv.org/abs/1804.02941) got accepted at WACV 2018 (Oral).
* Got selected for Dean's Merit List for Academic Excellence Award- 2015, 2016, 2017 & 2018
* Got Undergraduate Research Award for Outstanding Publications in Undergraduate years.

## <a name="publications" id="publications"> Publications

### Sampling Bias in Deep Active Classification: An Empirical Study [\[PDF\]](https://github.com/drimpossible/drimpossible.github.io/blob/master/documents/Active-Sampling-Bias.pdf) [\[Code\]](https://github.com/drimpossible/Sampling-Bias-Active-Learning)
<details> <summary><a>Abstract</a></summary>
The exploding cost and time needed for data labeling and model training are bottlenecks for training DNN models on large datasets. Identifying smaller representative data samples with strategies like active learning can help mitigate such bottlenecks. Previous works on active learning in NLP identify the problem of sampling bias in the samples acquired by uncertainty-based querying and develop costly approaches to address it. Using a large empirical study, we demonstrate that active set selection using the posterior entropy of deep models like FastText.zip (FTZ) is robust to sampling biases and to various algorithmic choices (query size and strategies) unlike that suggested by traditional literature. We also show that FTZ based query strategy produces sample sets similar to those from more sophisticated approaches (e.g ensemble net-
works). Finally, we show the effectiveness of the selected samples by creating tiny high quality datasets, and utilizing them for fast and cheap training of large models. Based on the above, we propose a simple baseline for deep active text classification that outperforms the state-of-the-art. We expect the presented work to be useful and informative for dataset compression and for problems involving active, semi-supervised or online learning scenarios.
</details>


### Exploring Binarization and Pruning for Convolutional Neural Networks [\[PDF\]](https://github.com/drimpossible/drimpossible.github.io/blob/master/documents/Thesis.pdf) [\[Slides\]](https://docs.google.com/presentation/d/1eNIW_jKwSaADMQb9chthUE6KgXaqYmUmhvO1cPeofrM/edit?usp=sharing)
<details> <summary><a>Abstract</a></summary>
Deep learning models have evolved remarkably, and are pushing the state-of-the-art in various problemsacross domains.  At the same time, the complexity and the amount of resources these DNNs consumehas greatly increased.  Today’s DNNs are computationally intensive to train and run, especially Con-volutional Neural Networks (CNNs) used for vision applications.  They also occupy a large amount ofmemory and consume a large amount of power during training.  This poses a major roadblock to thedeployment of such networks, especially in real-time applications or on resource-limited devices.  Twomethods have shown promise in compressing CNNs: (i) Binarization and (ii) Pruning. We explore thesetwo methods in this thesis.
</details>


### Deep Expander Networks: Efficient Deep Networks from Graph Theory, ECCV 2018 (Oral) [\[Talk\]](https://www.youtube.com/watch?v=0poGlFx1OBU) [\[PDF\]](http://openaccess.thecvf.com/content_ECCV_2018/papers/Ameya_Prabhu_Deep_Expander_Networks_ECCV_2018_paper.pdf) [\[Code\]](https://github.com/drimpossible/Deep-Expander-Networks) [\[Slides\]](https://github.com/drimpossible/drimpossible.github.io/blob/master/documents/AmeyaGirishAnoop_ExpanderECCV.pdf) [\[Poster\]](https://github.com/drimpossible/drimpossible.github.io/blob/master/documents/Expander_Poster.pdf)
<details> <summary><a>Abstract</a></summary>
Efficient CNN designs like ResNets and DenseNet were proposed to improve accuracy vs efficiency trade-offs. They essentially increased the connectivity, allowing efficient information flow across layers. Inspired by these techniques, we propose to model connections between filters of a CNN using graphs which are simultaneously sparse and well connected. Sparsity results in efficiency while well connectedness can preserve the expressive power of the CNNs. We use a well-studied class of graphs from theoretical computer science that satisfies these properties known as Expander graphs. Expander graphs are used to model connections between filters in CNNs to design networks called X-Nets. We present two guarantees on the connectivity of X-Nets: Each node influences every node in a layer in logarithmic steps, and the number of paths between two sets of nodes is proportional to the product of their sizes. We also propose efficient training and inference algorithms, making it possible to train deeper and wider X-Nets effectively.
Expander based models give a 4% improvement in accuracy on MobileNet over grouped convolutions, a popular technique, which has the same sparsity but worse connectivity. X-Nets give better performance trade-offs than the original ResNet and DenseNet-BC architectures. We achieve model sizes comparable to state-of-the-art pruning techniques using our simple architecture design, without any pruning. We hope that this work motivates other approaches to utilize results from graph theory to develop efficient network architectures.
</details>


### Hybrid Binary Networks: Optimizing for Accuracy, Efficiency and Memory, WACV 2018 (Oral) [\[PDF\]](https://arxiv.org/abs/1804.03867) [\[Talk\]](https://www.youtube.com/watch?v=QhhRkewA2r0) [\[Slides\]]() [\[Poster\]]() [\[Code\]]() 
---
